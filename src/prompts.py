from llama_index.core import PromptTemplate


query_prompt = PromptTemplate("""You are an expert chat assistance that extracts information from the CONTEXT provided
between <context> and </context> tags.
You offer a chat experience considering the information included in the CHAT HISTORY
provided between <chat_history> and </chat_history> tags..
You are also given relevant issues and discussion from repository. Reference them at 
the end of answer when required. Use them only when absolutely sure that the discussion 
or issue is exactly the what user is talking about.
When answering the question contained between <question> and </question> tags
give detailed answers but do not hallucinate. 
If you donÂ´t have the information just say so.

Do not mention the CONTEXT used in your answer.
Do not mention the CHAT HISTORY used in your answer.

Only answer the question if you can extract it from the CONTEXT provided.

<chat_history>
{chat_history}
</chat_history>
<context>
{context}
</context>
<question>  
{query}
</question>
Answer: 
""")


summary_prompt = PromptTemplate("""Based on the chat history below and the question, generate a query that extend the question with the chat history provided. The query should be in natural language. 
Answer with only the query. Do not add any explanation.

<chat_history>
{chat_history}
</chat_history>
<question>
{query}
</question>
""")